# -*- coding: utf-8 -*-
"""Operatis 1B- Transaction Matching Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DEa8ZJZeFsrKyqi1XfvpGqgx9Hqf-EYF

## Import libraries
"""



# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno # To visualize missing value
import plotly.graph_objects as go # To Generate Graphs
import plotly.express as px # To Generate box plot for statistical representation
import requests
# %matplotlib inline

"""# **Getting data**"""

# Uploading first CSV file (train file)
file_id = '1J4lYzBRExV_lOdMMMcKOlfWAIkKeI2qM'
url = f'https://drive.google.com/uc?id={file_id}'

# Read the CSV file into a DataFrame
train_df = pd.read_csv(url)
train_df.head()

# Upload the second CSV file (eval file)
file_id = '1OmYzjdIot7f0zB35qsET4XxcOvu6REea'
url = f'https://drive.google.com/uc?id={file_id}'

# Read the CSV file into a DataFrame
eval_df = pd.read_csv(url)
eval_df.head()

"""# **Data** **Information**"""

# print summary of train CSV
print(train_df.describe())
print(train_df.dtypes)
print(train_df.columns)

# print the shape (number of rows and columns) of the training df
train_df.shape

# Check for missing values in each column and calculate the sum of missing values
missing_values_1 = train_df.isna().sum()
print(missing_values_1)

"""Which columns do we need: Amount, date, Account(Number), transactionAttributes, **matchID**
Which columns can we drop: Everthing else.
"""

# matchRule is an unnecessary column for model training
train_df.drop(['matchRule'], axis=1)

# Find and print the unique values of each column

unique_vals = {}
for col in train_df.columns:
  unique_vals[col] = train_df[col].unique()

# print unique columns vals
for col, vals in unique_vals.items():
  print(f"Unique values in {col}:", vals)

# From the list of all unique values, refine our training dataframe even more.
irrelevant_columns = ['A_debitOrCredit', 'B_debitOrCredit','wasPreviouslyMismatched','matchedBy','A_currencyCode','B_currencyCode',]
train_df = train_df.drop(columns=irrelevant_columns)

# Check and drop rows without any data at all.
# Saw at the bottom of the sheet there were 2 rows copmletely blank, just to be sure it is not affecting the data
train_df.dropna(how='all', inplace=True)

# Check for duplicate records (Should I just remove them?)
duplicate_rows = train_df[train_df.duplicated()]

# Count duplicates
num_duplicate_rows = duplicate_rows.shape[0]

print(duplicate_rows.shape)
print("Total count:", num_duplicate_rows)

print(train_df.head())
print(train_df.shape)

print(train_df.columns)

"""**Dividing the original "train_df" into 2 separate dataframes for binary classification.**

A_recon_df: Dataframe that holds rows that exclusively labeled artififact "A".

B_recon_df: Dataframe that holds rows exclusively labeled artifact "B".
"""

# A_recon_df: select rows that have value "A" under A_transactionType into a separate dataframe.
filter_val = 'A'

# drop the non A transaction columns
A_train_df = train_df.drop(['B_transactionType','B_id','B_importDate',
                            'B_amount','B_valueDate','B_account',
                            'B_transactionReferences','B_transactionAttributes',
                            'targetAllocation'], axis=1);

# create dataframe exclusively for A if the row is an A type
A_recon_df = A_train_df[(A_train_df['A_transactionType'] == filter_val)]
print(A_recon_df.shape)

# B_recon_df: select rows that have value "B" under B_transactionType into a separate dataframe.
filter_val = 'B'

# drop the non A transaction columns
B_train_df = train_df.drop(['A_transactionType', 'A_id','A_allocation',
                            'A_importDate', 'A_amount', 'A_valueDate',
                            'A_account', 'A_transactionReferences',
                            'A_transactionAttributes'], axis=1);

# create dataframe exclusively for A if the row is an A type
B_recon_df = B_train_df[(B_train_df['B_transactionType'] == filter_val)]
print(B_recon_df.shape)

A_recon_df.isna().sum()

B_recon_df.isna().sum()

"""### **Create an Array to Store All Combinations of A and B Transactions**"""

# add indices to A_recon_df and B_recon_df
# by resetting the index to default integer indices
A_recon_df.reset_index(inplace=True)
B_recon_df.reset_index(inplace=True)

"""An issue I have with running the code in the cell below is the amount of running time required. Because of the sheer magnitude of the number of transactions in the data, finding all the combinations is very time-consuming. Thus, I decided to use the code cell after, which is the same except I limited the transactions to only 100 each of A and B. (I chose the number 100 arbitarily; it just seemed like a decent number to work with). With the smaller subset of 100, the code ran quickly and performed as expected."""

# DO NOT RUN THIS CELL! RUN THE NEXT ONE - SEE NOTE ABOVE
# represent each transaction as an index in A_recon_df or B_recon_df
# to create an array of the combinations iteratively
combinations = [];
# use for loop to iterate over each A transaction
for i in range(100):
  # for every A transaction, create a pair with every B transaction
  for j in range(100):
    # compare each of the corresponding transaction attributes and
    # assign a numeric value 0 or 1 meaning equal or unequal
    # by appending it into an array
    booleanNumericValues = [];

    # first compare matchId
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchId')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchId')]:
      booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # second compare matchDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # third compare matchDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchRule')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchRule')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # fourth compare transactionType
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionType')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionType')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # fifth compare id
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_id')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_id')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # sixth compare allocation
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_allocation')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('targetAllocation')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # seventh compare importDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_importDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_importDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # eighth compare amount
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_amount')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_amount')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # ninth compare valueDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_valueDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_valueDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # tenth compare account
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_account')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_account')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # eleventh compare transactionReferences
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionReferences')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionReferences')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # twelfth compare transactionAttributes
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionAttributes')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionAttributes')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)
    combinations.append(booleanNumericValues)

combinations[0]

# represent each transaction as an index in A_recon_df or B_recon_df
# to create an array of the combinations iteratively
combinations = [];
# use for loop to iterate over each A transaction
for i in range(100):
  # for every A transaction, create a pair with every B transaction
  for j in range(100):
    # compare each of the corresponding transaction attributes and
    # assign a numeric value 0 or 1 meaning equal or unequal
    # by appending it into an array
    booleanNumericValues = [];

    # first compare matchId
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchId')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchId')]:
      booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # second compare matchDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # third compare matchDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('matchRule')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('matchRule')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # fourth compare transactionType
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionType')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionType')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # fifth compare id
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_id')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_id')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # sixth compare allocation
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_allocation')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('targetAllocation')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # seventh compare importDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_importDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_importDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # eighth compare amount
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_amount')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_amount')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # ninth compare valueDate
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_valueDate')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_valueDate')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # tenth compare account
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_account')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_account')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # eleventh compare transactionReferences
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionReferences')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionReferences')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)

    # twelfth compare transactionAttributes
    if A_recon_df.iloc[i, A_recon_df.columns.get_loc('A_transactionAttributes')] == B_recon_df.iloc[j, B_recon_df.columns.get_loc('B_transactionAttributes')]:
     booleanNumericValues.append(1)
    else:
      booleanNumericValues.append(0)
    combinations.append(booleanNumericValues)

# create a label dataframe as input for training the model
# the label is the 0/1 value that indicates
# if two transactions (one A and one B) is a match

label_df = pd.DataFrame(combinations)
label_df.head()

"""# **Training a Basic Decision Tree model**

Using the newly generated label dataframe and the transaction data characteristics as features, we train a basic decision tree model for evaluation before continuing with a random forest model.
"""

# initialize the label into the variable y
# the label is the value 0/1, indicating no/yes match

y = label_df[0]
print(y)
X = label_df.drop(columns=[0], axis=1)
print(X)

# Import additional packages
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import StackingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_curve

# make data subsets for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.30, random_state=1234)

print(X_train.dtypes)
print()
print(y_train.dtypes)

# Initialize a DecisionTreeRegressor model object with the default value of hyperparameter C
# The model object should be named 'model' with default hyperparameters
dt_model = DecisionTreeRegressor()

# Fit the model to the new training data
dt_model.fit(X_train, y_train)

# Use the predict() method to use your model to make predictions on the new test data
# Save the values of the second column to a list called 'proba_predictions'
proba_predictions = dt_model.predict(X_test)

# Compute the rmse and r2
dt_rmse = mean_squared_error(y_test, proba_predictions, squared=False)
dt_r2 =  r2_score(y_test, proba_predictions)


print('[DT] Root Mean Squared Error: {0}'.format(dt_rmse))
print('[DT] R2: {0}'.format(dt_r2))

"""### Random Forest Model

pending work - I have not worked on this model yet. The code below is earlier work by Joaquin.
"""

# train,test,split for RF model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=34)

print(X.shape)
print(y.shape)

print('Begin Operartis1B Random Forest Implementation...')
print('Starting n_estimators=20...')


# 1. Create the RandomForestClassifier model object with criterion='entropy' and n_estimators=20
rf_20_model = RandomForestClassifier(criterion='entropy', n_estimators=20)

# 2. Fit the model to the training data
rf_20_model.fit(X_train, y_train)

# 3. Make predictions on the test data and assign the result to 'rf_20_predictions'
rf_20_predictions = list(rf_20_model.predict_proba(X_test)[:, 1])

rf_100_model = RandomForestClassifier(criterion='entropy', n_estimators=100)

rf_100_model.fit(X_train, y_train)

rf_100_predictions = list(rf_100_model.predict_proba(X_test)[:, 1])


print('End')

print(rf_20_predictions)

# END...Everyting after this point was testing possible solutions.

# Tried using a possible solution , but this is not working well either... I also have to take a deeper dive.

from sklearn.model_selection import GridSearchCV

# Create a Random Forest Classifier
rf_model = RandomForestClassifier()

# Define a range of hyperparameter values to search over
param_grid = {
    'n_estimators': [100],
    'max_depth': [10],
    'min_samples_split': [2],
    'min_samples_leaf': [1],
    'max_features': ['auto'],
}

# Create a grid search object
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameter values and the corresponding model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Use the best model for prediction
y_pred = best_model.predict(X_test)

features_to_drop = ['matchId', 'matchDate', 'matchRule', 'matchedBy', 'wasPreviouslyMi
smatched', 'generatorArtifactName', 'generatorId', 'generatorImportDate', 'generatedAr
tifactName', 'generatedId', 'generatedImportDate']
X_train = train_data.drop(features_to_drop + ['targetAllocation'], axis=1)
y_train = train_data['targetAllocation']
X_test = test_data.drop(features_to_drop + ['targetAllocation'], axis=1)
y_test = test_data['targetAllocation']
# Convert categorical variables to numerical using one-hot encoding
encoder = OneHotEncoder()
X_train_encoded = encoder.fit_transform(X_train)

y = df['targetAllocation']

clf = RandomForestClassifier(n_estimators=20, random_state=42)
clf.fit(X, y)
# Prediction on the testing dataset
y_pred = clf.predict(X)
# Evaluate the model
accuracy = accuracy_score(y, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
# To make predictions on new data:
# new_data_encoded = encoder.transform(new_data)
# predictions = clf.predict(new_data_encoded)

